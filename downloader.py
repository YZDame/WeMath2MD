import requests
from bs4 import BeautifulSoup
import os
import time
import re
from pathlib import Path

class WechatImageDownloader:
    def __init__(self, output_dir="output"):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨
        
        Args:
            output_dir: åŸºç¡€è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸º "output"
        """
        self.output_dir = Path(output_dir)
        self.article_title = None
        self.result_dir = None  # æœ€ç»ˆç»“æœç›®å½•ï¼ˆä»¥æ ‡é¢˜å‘½åï¼‰
        self.images_dir = None  # å›¾ç‰‡ä¿å­˜ç›®å½•
        
        # ä¼ªè£…æˆæµè§ˆå™¨ï¼Œé˜²æ­¢åçˆ¬
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

    def get_article_content(self, url):
        """è·å–ç½‘é¡µHTMLå†…å®¹"""
        try:
            response = requests.get(url, headers=self.headers)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"âŒ è¯·æ±‚æ–‡ç« å¤±è´¥: {e}")
            return None
    
    def extract_title(self, html_content):
        """ä»HTMLä¸­æå–æ–‡ç« æ ‡é¢˜"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # å°è¯•å¤šç§æ–¹å¼è·å–æ ‡é¢˜
        title = None
        
        # æ–¹å¼1: å¾®ä¿¡å…¬ä¼—å·ç‰¹å®šæ ‡ç­¾
        title_tag = soup.find(id="activity-name")
        if title_tag:
            title = title_tag.get_text(strip=True)
        
        # æ–¹å¼2: og:title meta æ ‡ç­¾
        if not title:
            og_title = soup.find('meta', property='og:title')
            if og_title:
                title = og_title.get('content', '').strip()
        
        # æ–¹å¼3: æ ‡å‡† title æ ‡ç­¾
        if not title:
            title_tag = soup.find('title')
            if title_tag:
                title = title_tag.get_text(strip=True)
        
        # æ¸…ç†æ ‡é¢˜ä¸­çš„éæ³•å­—ç¬¦ï¼ˆç”¨äºæ–‡ä»¶åï¼‰
        if title:
            # ç§»é™¤æˆ–æ›¿æ¢æ–‡ä»¶åéæ³•å­—ç¬¦
            title = re.sub(r'[\\/*?:"<>|]', '_', title)
            # ç§»é™¤å¤šä½™ç©ºæ ¼
            title = re.sub(r'\s+', ' ', title).strip()
            # é™åˆ¶é•¿åº¦ï¼ˆé¿å…æ–‡ä»¶åè¿‡é•¿ï¼‰
            if len(title) > 50:
                title = title[:50]
        
        return title or f"article_{int(time.time())}"
    
    def setup_directories(self, title):
        """æ ¹æ®æ ‡é¢˜è®¾ç½®ç›®å½•ç»“æ„"""
        self.article_title = title
        
        # åˆ›å»ºç»“æœç›®å½•: output/{æ ‡é¢˜}/
        self.result_dir = self.output_dir / title
        self.result_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå›¾ç‰‡å­ç›®å½•: output/{æ ‡é¢˜}/downloaded_images/
        self.images_dir = self.result_dir / "downloaded_images"
        self.images_dir.mkdir(exist_ok=True)
        
        print(f"ğŸ“ ç»“æœç›®å½•: {self.result_dir}")
        return self.result_dir

    def extract_images(self, html_content):
        """ä»HTMLä¸­æå–æ‰€æœ‰å›¾ç‰‡é“¾æ¥"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # å…¬ä¼—å·æ­£æ–‡é€šå¸¸åœ¨ id="img-content" æˆ– class="rich_media_content" ä¸­
        content = soup.find(id="img-content")
        if not content:
            content = soup.find(class_="rich_media_content")
            
        if not content:
            print("âš ï¸ æœªæ‰¾åˆ°æ­£æ–‡å†…å®¹")
            return []

        images = []
        # å¾®ä¿¡å…¬ä¼—å·å›¾ç‰‡çš„çœŸå®é“¾æ¥é€šå¸¸åœ¨ data-src ä¸­
        for img in content.find_all('img'):
            src = img.get('data-src')
            if src:
                # è¿‡æ»¤æ‰ä¸€äº›å›¾æ ‡æˆ–æ— æ•ˆå›¾ç‰‡ï¼ˆå¯é€‰ï¼‰
                images.append(src)
                
        return images

    def download_images(self, img_urls):
        """ä¸‹è½½å›¾ç‰‡åˆ—è¡¨"""
        if self.images_dir is None:
            raise Exception("è¯·å…ˆè°ƒç”¨ setup_directories() è®¾ç½®ç›®å½•")
        
        saved_files = []
        print(f"ğŸ” æ‰¾åˆ° {len(img_urls)} å¼ å›¾ç‰‡ï¼Œå‡†å¤‡ä¸‹è½½...")
        
        for index, url in enumerate(img_urls):
            try:
                # è·å–å›¾ç‰‡æ ¼å¼
                fmt = "jpg" # é»˜è®¤ä¸ºjpg
                if "fmt=" in url:
                    fmt_match = re.search(r'fmt=([a-zA-Z]+)', url)
                    if fmt_match:
                        fmt = fmt_match.group(1)
                
                # æ„é€ æ–‡ä»¶åï¼š001.jpg, 002.png ...
                filename = f"{index+1:03d}.{fmt}"
                filepath = self.images_dir / filename
                
                # ä¸‹è½½
                img_resp = requests.get(url, headers=self.headers)
                with open(filepath, 'wb') as f:
                    f.write(img_resp.content)
                
                print(f"âœ… å·²ä¸‹è½½ [{index+1}/{len(img_urls)}]: {filename}")
                saved_files.append(str(filepath))
                
                # ç¤¼è²Œæ€§å»¶æ—¶
                time.sleep(0.5) 
                
            except Exception as e:
                print(f"âŒ ä¸‹è½½ç¬¬ {index+1} å¼ å›¾ç‰‡å¤±è´¥: {e}")
        
        return saved_files
    
    def download_from_url(self, url):
        """
        ä¸€ç«™å¼ä¸‹è½½ï¼šä» URL è·å–æ–‡ç« ï¼Œæå–æ ‡é¢˜ï¼Œä¸‹è½½å›¾ç‰‡
        
        Returns:
            dict: {
                'title': æ–‡ç« æ ‡é¢˜,
                'result_dir': ç»“æœç›®å½•è·¯å¾„,
                'images_dir': å›¾ç‰‡ç›®å½•è·¯å¾„,
                'images': ä¸‹è½½çš„å›¾ç‰‡æ–‡ä»¶åˆ—è¡¨
            }
        """
        print(f"ğŸ”— æ­£åœ¨è·å–æ–‡ç« : {url}")
        
        # 1. è·å– HTML
        html = self.get_article_content(url)
        if not html:
            return None
        
        # 2. æå–æ ‡é¢˜
        title = self.extract_title(html)
        print(f"ğŸ“° æ–‡ç« æ ‡é¢˜: {title}")
        
        # 3. è®¾ç½®ç›®å½•
        self.setup_directories(title)
        
        # 4. æå–å¹¶ä¸‹è½½å›¾ç‰‡
        img_urls = self.extract_images(html)
        saved_files = self.download_images(img_urls)
        
        print(f"\nâœ… ä¸‹è½½å®Œæˆï¼å…± {len(saved_files)} å¼ å›¾ç‰‡")
        print(f"ğŸ“ ä¿å­˜ä½ç½®: {self.images_dir}")
        
        return {
            'title': title,
            'result_dir': str(self.result_dir),
            'images_dir': str(self.images_dir),
            'images': saved_files
        }


# --- æµ‹è¯•ä»£ç  ---
if __name__ == "__main__":
    # è¿™é‡Œæ¢æˆä¸€ä¸ªçœŸå®çš„å…¬ä¼—å·æ–‡ç« é“¾æ¥è¿›è¡Œæµ‹è¯•
    test_url = "https://mp.weixin.qq.com/s/0FKXBV81FzHcd4QcHTVvHg" 
    
    if "ä½ çš„æµ‹è¯•æ–‡ç« é“¾æ¥" in test_url:
        print("âš ï¸ è¯·å…ˆæ›¿æ¢ä»£ç åº•éƒ¨çš„ test_url ä¸ºçœŸå®çš„å…¬ä¼—å·æ–‡ç« é“¾æ¥ï¼")
    else:
        downloader = WechatImageDownloader(output_dir="output")
        result = downloader.download_from_url(test_url)
        
        if result:
            print(f"\nğŸ“Š ä¸‹è½½ç»“æœ:")
            print(f"   æ ‡é¢˜: {result['title']}")
            print(f"   ç»“æœç›®å½•: {result['result_dir']}")
            print(f"   å›¾ç‰‡ç›®å½•: {result['images_dir']}")
            print(f"   å›¾ç‰‡æ•°é‡: {len(result['images'])}")
